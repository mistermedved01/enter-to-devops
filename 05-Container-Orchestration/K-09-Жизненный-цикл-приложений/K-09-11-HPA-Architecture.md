[🏠 Главная](../../README.md) → [☸️ Container-Orchestration](../../README.md#-container-orchestration) → [🔄 K-09-Жизненный-цикл-приложений](../../README.md#-k-09-жизненный-цикл-приложений)

---

# 📊K-09-11-HPA-Architecture
>Архитектура Horizontal Pod Autoscaler (HPA): компоненты, типы метрик (Resource, Custom, External), Metrics Server, адаптеры, цикл работы, настройка поведения

---

<details>
<summary><b>📊Введение: что такое HPA</b></summary>

---

## О чем эта лекция

Привет, в этом видео мы поговорим об архитектуре горизонтального скейлера и о том, как нам с ним работать.

В основе HPA лежит динамическая регулировка количества реплик в ReplicaSet Deployment'а.

Таким образом, этот механизм патчит поле `replicas` в базе объектов Kubernetes, а выкаткой дополнительных PODs или удалением лишних занимаются профильные механизмы Kubernetes.

---

## Поддерживаемые типы рабочих нагрузок

HPA работает не только с типом Deployment, но и с StatefulSet.

---

## Основные метрики

Традиционно HPA опирается на две основные метрики, когда принимает решение о скалировании. Это две самые важные вещи, и они много где используются в кластере:

1. **Потребление CPU** - иными словами, достаточно ли нашему приложению ресурсов процессора
2. **Память (Memory)** - достаточно ли приложению памяти

В общем-то, в очень многих случаях этого достаточно, чтобы поддерживать нагрузку в лучшем состоянии.

---

## Дополнительные метрики

Но что если приложению нужно больше, чем просто управление по метрикам, которые реализованы внутри Kubernetes?

Скажем, нам нужно отслеживать:
- Количество активных пользователей
- Сколько заказов в данный момент в обработке

К счастью, HPA позволяет подключить к себе ряд дополнительных метрик. Это пользовательские и внешние метрики, и они обеспечат нам необходимую гибкость.

---

</details>

<details>
<summary><b>🏗️Компоненты HPA</b></summary>

---

## Составные части HPA

Прежде чем настраивать это, давай разберемся с внутренним составом. Он состоит из нескольких компонентов:

1. **Определение ресурса HPA** - файл определения Kubernetes
2. **Metrics Server** - компонент для сбора встроенных метрик
3. **Адаптеры метрик** - для пользовательских и внешних метрик

### Общая схема архитектуры HPA

```
┌─────────────────────────────────────────────────────────┐
│  HPA (Horizontal Pod Autoscaler)                        │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Определение ресурса HPA (манифест)              │   │
│  │  - Целевая рабочая нагрузка                       │   │
│  │  - Правила масштабирования                        │   │
│  │  - Источник метрик                                │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Metrics API (metrics.k8s.io)                     │   │
│  │  - Resource metrics (CPU/Memory)                  │   │
│  │  - Custom metrics (custom.metrics.k8s.io)        │   │
│  │  - External metrics (external.metrics.k8s.io)    │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│        ┌───────────────┴───────────────┐               │
│        ▼                               ▼               │
│  ┌──────────────┐            ┌──────────────────┐     │
│  │ Metrics      │            │ Адаптеры метрик   │     │
│  │ Server       │            │ (Prometheus, etc) │     │
│  │              │            │                  │     │
│  │ Собирает     │            │ Собирает         │     │
│  │ CPU/Memory   │            │ кастомные/       │     │
│  │ из kubelet   │            │ внешние метрики  │     │
│  └──────────────┘            └──────────────────┘     │
│        │                               │               │
│        └───────────────┬───────────────┘               │
│                        ▼                               │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Deployment / StatefulSet                        │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  POD 1, POD 2, POD 3, ...                   │ │   │
│  │  │  (количество реплик управляется HPA)         │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

---

</details>

<details>
<summary><b>📝Определение ресурса HPA</b></summary>

---

## Что такое HPA ресурс

Сначала поговорим об определении ресурса Horizontal Pod Autoscaler.

На самом деле это план, где мы описываем масштабирование. Он определяет, как развертыванию (то есть наши рабочие нагрузки) понимать, что пора скейлиться.

---

## Структура манифеста HPA

Если мы посмотрим на YAML файл, то заметим, что целевая рабочая нагрузка в данном случае - Deployment.

Мы также указываем минимальное и максимальное количество копий приложения под полями со словом `replicas`.

### Пример базового HPA

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 8
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

Данные параметры установили коридор, в котором количество реплик в самом спокойном состоянии будет уменьшено до двух, а при максимальной нагрузке мы позволили развернуться до восьми PODs.

---

## Метрики для масштабирования

Далее описываются метрики, опираясь на которые будет приниматься решение о масштабировании.

Пока мы говорим только о встроенных в кластер метриках. В показанном случае он будет следить за количеством CPU, которые потребляют PODs.

Если для наших приложений утилизация в среднем составляет более пятидесяти процентов, то это сигнал к добавлению экземпляров.

---

## Цикл работы HPA

После создания контроллера горизонтального скейлера начнет отслеживать целевые PODs.

Таким образом, HPA будет постоянно на страже и станет проверять метрику CPU каждые пятнадцать секунд.

### Схема цикла работы HPA

```
┌─────────────────────────────────────────────────────────┐
│  Цикл HPA (каждые 15 секунд)                            │
│  ┌──────────────────────────────────────────────────┐   │
│  │  1. Получение метрик                             │   │
│  │     ┌──────────────────────────────────────────┐ │   │
│  │     │  Запрос к Metrics API                    │ │   │
│  │     │  Получение CPU/Memory для PODs           │ │   │
│  │     └──────────────────────────────────────────┘ │   │
│  │                                                   │   │
│  │  2. Оценка метрик                                │   │
│  │     ┌──────────────────────────────────────────┐ │   │
│  │     │  Сравнение с пороговыми значениями        │ │   │
│  │     │  Текущая утилизация: 70%                  │ │   │
│  │     │  Целевая утилизация: 50%                  │ │   │
│  │     │  ✅ Превышен порог                        │ │   │
│  │     └──────────────────────────────────────────┘ │   │
│  │                                                   │   │
│  │  3. Расчет масштабирования                       │   │
│  │     ┌──────────────────────────────────────────┐ │   │
│  │     │  Текущие реплики: 3                      │ │   │
│  │     │  Расчет: 3 * (70% / 50%) = 4.2          │ │   │
│  │     │  Новые реплики: 4 (округление)           │ │   │
│  │     └──────────────────────────────────────────┘ │   │
│  │                                                   │   │
│  │  4. Обновление Deployment                        │   │
│  │     ┌──────────────────────────────────────────┐ │   │
│  │     │  Патч поля replicas в Deployment         │ │   │
│  │     │  Deployment создает новые PODs           │ │   │
│  │     └──────────────────────────────────────────┘ │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

---

</details>

<details>
<summary><b>📈Metrics Server</b></summary>

---

## Откуда HPA получает метрики?

Но откуда именно он получает эти метрики? С утилизация CPU PODs Deployment'а?

У Kubernetes есть специальный API для сбора внутренних метрик с PODs и нод. Но сам API-сервер не умеет собирать показания, он только умеет их передавать.

Поэтому для работы он полагается на компонент Metrics Server, который необходимо развернуть.

---

## Как работает Metrics Server

Сервер метрик обращается в kubelet и опрашивает его о потреблении CPU и памяти контейнерами Deployment'а.

По сути, такой Metrics Server - must-have для любого производственного кластера. Это потому, что метрики процессора и памяти критически важны для понимания того, что именно происходит в кластере.

Именно эти числа определяют, масштабируемся мы или нет. Но они могут быть задействованы и в других механизмах управления кластером, не только в HPA.

### Схема работы Metrics Server

```
┌─────────────────────────────────────────────────────────┐
│  Metrics Server                                         │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Периодически опрашивает kubelet на каждом узле  │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  Node 1: kubelet                            │ │   │
│  │  │  ├── POD: app-1 (CPU: 200m, Memory: 256Mi)  │ │   │
│  │  │  └── POD: app-2 (CPU: 150m, Memory: 128Mi)  │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  │                                                   │   │
│  │  Node 2: kubelet                                  │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  POD: app-3 (CPU: 180m, Memory: 200Mi)      │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Предоставляет данные через Metrics API          │   │
│  │  (metrics.k8s.io)                                 │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  HPA запрашивает метрики                          │   │
│  │  Получает: средняя утилизация CPU по PODs        │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

---

## Важность Metrics Server

Поэтому, чтобы HPA работал, ему надо предоставить данные рабочих нагрузок из Metrics Server.

Именно вычитанные данные, сравнивая с пороговыми значениями, HPA принимает решение, как корректировать количество реплик Deployment'а.

Еще раз: HPA скейлит Deployment, добавляя и удаляя PODs на основе данных из кластерного API, который получает их из сервера метрик.

Это важно, потому что без метрик манифест не будет работать, даже если мы его создадим в кластере.

---

</details>

<details>
<summary><b>🔍Как HPA обрабатывает метрики</b></summary>

---

## Обработка метрик для каждой реплики

В нашем манифесте мы указали в качестве метрики CPU, и это значит, что HPA интересует каждая реплика Deployment'а, то есть каждый его POD.

А в PODе - каждый из контейнеров. Он вычитает значение процессоров для каждого контейнера, усреднит и проверит, как это соотносится с пороговым значением.

Если рассчитанный показатель больше порога, то добавит реплик.

---

## Проблема с мульти-контейнерными PODs

Но в PODе много контейнеров - это может быть неточным.

Поэтому, кроме `Resource`, как ты видишь, обычные метрики могут работать с типом `Container`.

### Пример с Container метрикой

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 8
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
      container: web-server  # Указываем конкретный контейнер
```

Это по сути то же самое, но только для определенного контейнера в PODе. Это важно в мульти-контейнерных PODах, когда только один из контейнеров нужно брать во внимание при масштабировании.

---

</details>

<details>
<summary><b>🎨Пользовательские метрики (Custom Metrics)</b></summary>

---

## Когда нужны пользовательские метрики

Теперь давай посмотрим, как обстоит дело, если решили использовать не CPU, а свои собственные метрики.

Как нам описать, что мы хотим извлечь метрики из кастомного источника?

---

## Требования для пользовательских метрик

Во-первых, наше приложение должно уметь отдавать эти метрики. Следовательно, разработчики должны его инструментировать.

Во-вторых, нам понадобится источник метрик, который будет действовать внутри кластера и снимать метрики с PODs или других Kubernetes объектов.

Этот источник называется **адаптером пользовательских метрик** и по сути он работает как Metrics Server, но для других типов метрик.

### Схема работы пользовательских метрик

```
┌─────────────────────────────────────────────────────────┐
│  Приложение в POD                                        │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Инструментированное приложение                  │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  Отдает метрики:                            │ │   │
│  │  │  - requests_per_second: 45                  │ │   │
│  │  │  - active_users: 120                        │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Адаптер пользовательских метрик                 │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  Периодически извлекает метрики             │ │   │
│  │  │  из приложения                               │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Custom Metrics API                               │   │
│  │  (custom.metrics.k8s.io)                         │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  HPA запрашивает метрики                          │   │
│  │  Получает: requests_per_second для PODs          │   │
│  │  Сравнивает с порогом: 50 req/s                  │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

---

## Важное замечание

Отметь, что сервер метрик мы не убираем, так как без него не будут работать нативные метрики. Просто мы устанавливаем еще один сервер, который умеет работать с дополнительными типами метрик.

Скажем, наша задача - собрать с PODs информацию, сколько в секунду у них прилетает запросов.

Таким образом, адаптер будет периодически извлекать метрики из приложения. В свою очередь, HPA будет обращаться к кластеру за метрикой, а в ответ получать данные, которые были извлечены из этого адаптера пользовательских метрик.

Далее, как обычно, HPA их просмотрит и скорректирует реплики на основе этой информации.

---

## Пример манифеста с пользовательскими метриками

Вот здесь он проверяет, что в среднем на PODе в секунду было не более пятидесяти запросов.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 8
  metrics:
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "50"
```

Таким образом, мы настраиваем значения для пользовательских метрик прямо в манифесте.

---

## Важно помнить

Важно помнить, что пользовательские метрики не собираются при помощи Metrics Server, но все еще остаются внутренними метриками кластера.

Просто они нативно не поддерживаются, и нам приходится вставлять специальный адаптер в цепочку.

В остальном это такой же манифест, как и раньше. У нас будет целевой объект (Deployment), коридор конечных значений для реплик, но в метриках будет изменение.

---

## Типы пользовательских метрик

Здесь у нас `type: Pods` - это означает, что данные были сняты для PODs.

Другой возможный вариант для типа - это `Object`. Таким образом можно описать, что метрики относятся к определенному объекту Kubernetes.

В нашем случае мы работаем с кастомной метрикой внутри кластера, которая отображается через адаптер, а данные поступают из POD.

Мы рассматриваем среднее значение, установленное в пятьдесят. Это значит, что если усредненная по PODам метрика превышает порог, то HPA будет действовать.

---

</details>

<details>
<summary><b>🌐Внешние метрики (External Metrics)</b></summary>

---

## Что такое внешние метрики

Сейчас мы знаем нативный способ, который работает для CPU и Memory, и кастомный, который связан с указанными ресурсами кластера и требует адаптера для перевода метрик в формат Kubernetes Metrics API.

Но есть еще один, третий по счету тип - это внешние метрики.

В чем разница? На самом деле названия отражают суть: они являются внешними по отношению к кластеру и никак не связаны с объектами Kubernetes.

---

## External Metrics Adapter

Мы получаем к ним доступ через External адаптер. Этот адаптер фактически взаимодействует с внешним источником данных, собирает оттуда метрики и формирует ответ подходящего вида.

За адаптером может стоять любая служба:
- Это может быть приложение в другом кластере
- Это может быть решение по сбору метрик вроде Prometheus или DataDog
- Или поставщик метрик какого-то облачного провайдера

То есть нам не нужно владеть этим сервисом и развертывать его в своей инфраструктуре. Достаточно того, что мы можем обратиться к нему из кластера.

Как именно работает система на удаленной стороне, не интересует. Все это возложено на адаптер.

### Схема работы внешних метрик

```
┌─────────────────────────────────────────────────────────┐
│  Внешний источник данных                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  AWS SQS (очередь сообщений)                     │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  Сообщений в очереди: 15                    │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  External Metrics Adapter                         │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  Запрашивает метрики из AWS SQS             │ │   │
│  │  │  Преобразует в формат Kubernetes             │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  External Metrics API                            │   │
│  │  (external.metrics.k8s.io)                       │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  HPA запрашивает метрику                         │   │
│  │  Получает: sqs_queue_length = 15                │   │
│  │  Сравнивает с порогом: 10                        │   │
│  │  ✅ Превышен порог → масштабирование             │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

---

## Пример манифеста с внешними метриками

Как и в случае с предыдущими типами метрик, HPA будет проверять свои пороговые значения, сравнивать с метрикой и менять количество реплик целевой нагрузки.

Обрати внимание, что тип здесь установлен как `External`. Название метрики - это фактически скейлинг, завязанный на данных из очереди сообщений облака AWS.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: External
    external:
      metric:
        name: sqs_queue_length
      target:
        type: Value
        value: "10"
```

Это название передается в адаптер, и когда HPA хочет проверить данные, он запрашивает имя метрики.

Запрос идет через адаптер, который извлекает эту информацию. Сама информация находится в облаке Amazon. Адаптер сходит за ней и сформирует ответ, который вернется в HPA.

Таким образом, если в очереди больше десяти сообщений, то мы начинаем скалироваться.

---

</details>

<details>
<summary><b>🔌API группы для метрик</b></summary>

---

## Единая точка входа

Окей, все это многообразие типов завязано на единой точке входа для метрик, а именно на получении информации от API-сервера Kubernetes.

Он выставляет метрики на трех разных группах API:

### 1. metrics.k8s.io

Относится к серверу метрик. То есть это стандартный механизм, по которому можно получить метрики CPU и памяти для PODs и нод.

### 2. custom.metrics.k8s.io

Пользовательские метрики доступны в группе `custom.metrics.k8s.io`. Это подходит для приложений, размещенных внутри кластера.

То есть подразумевается, что метрики соотносятся с объектом Kubernetes, который принадлежит кластеру.

### 3. external.metrics.k8s.io

Внешние метрики занимают `external.metrics.k8s.io`. Они будут получены извне кластера и не связаны с объектами этого кластера.

### Схема API групп

```
┌─────────────────────────────────────────────────────────┐
│  Kubernetes API Server                                   │
│  ┌──────────────────────────────────────────────────┐   │
│  │                                                   │   │
│  │  metrics.k8s.io                                   │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  Resource metrics (CPU/Memory)              │ │   │
│  │  │  Источник: Metrics Server                   │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  │                                                   │   │
│  │  custom.metrics.k8s.io                           │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  Custom metrics (внутри кластера)            │ │   │
│  │  │  Источник: Custom Metrics Adapter           │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  │                                                   │   │
│  │  external.metrics.k8s.io                         │   │
│  │  ┌─────────────────────────────────────────────┐ │   │
│  │  │  External metrics (вне кластера)             │ │   │
│  │  │  Источник: External Metrics Adapter         │ │   │
│  │  └─────────────────────────────────────────────┘ │   │
│  │                                                   │   │
│  └──────────────────────────────────────────────────┘   │
│                        │                                 │
│                        ▼                                 │
│  ┌──────────────────────────────────────────────────┐   │
│  │  HPA запрашивает метрики через API                │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

---

## Адаптеры метрик

Для поддержки таких разных типов нам нужно развернуть адаптер метрик, который служит мостом для API Kubernetes.

Именно он говорит с API и реализацией внешнего решения мониторинга.

Поэтому точно так же, как простые метрики не будут работать без Metrics Server, то более сложные не заведутся без установленных адаптеров.

Именно адаптер занимается переводом разных типов метрик на язык, понятный Kubernetes, и поэтому наши манифесты HPA остаются простыми.

---

## Metrics Server как первый адаптер

Сервер метрик можно назвать самым первым таким адаптером, который собирает данные из kubelet и передает к ним доступ через Kubernetes API.

---

## Разница типов метрик

Разница типов метрик заключается в том, что:
- **Внешние метрики** прибывают из внешних систем
- **Кастомные** находятся внутри кластера, является либо объектом в кластере, либо приложением в кластере

Но оба типа требуют подходящего адаптера.

---

## Prometheus как универсальный адаптер

Метрика на самом деле популярные адаптеры вроде Prometheus могут работать и в качестве внутреннего, и внешнего адаптера.

Более того, любую внутреннюю метрику можно опубликовать на API внешней. Просто мы лишимся некоторых кластерных преимуществ, связанных с внутренней метрикой, но их по-прежнему можно будет использовать.

Еще раз: API внешнего типа - это `external.metrics.k8s.io`, для внутреннего типа - `custom.metrics.k8s.io`.

---

</details>

<details>
<summary><b>⚙️Цикл управления HPA</b></summary>

---

## Что следует принять во внимание

Окей, что следует принять во внимание, выбирая тип метрик?

У HPA есть свой цикл управления, который получает метрики и сверяется с установленными системе значениями.

По умолчанию он делает это каждые пятнадцать секунд. Этот интервал можно настроить на kube-controller-manager, но мы будем считать, что мы его не меняли.

Таким образом, каждые пятнадцать секунд он будет проверять и корректировать среду, если в этом есть потребность.

---

## Порядок цикла

Порядок его цикла такой:

1. **Получение метрик** - сначала он получает метрики
2. **Оценка метрик** - потом оценивает метрики, основываясь на установленном в настройках значении
3. **Расчет масштабирования** - если принято решение скейлиться, он рассчитывает, на какое именно значение реплик
4. **Обновление Deployment** - затем он обновляет Deployment рабочей нагрузки

Еще раз: каждые пятнадцать секунд он получит метрики и оценит их на основе пороговых значений. Если пороговое значение превышены, он выполнит расчет масштабирования, а затем обновит реплики с учетом рекомендаций по масштабированию.

---

</details>

<details>
<summary><b>⚠️Важные моменты и ограничения</b></summary>

---

## Узкие места

Напоследок, некоторые узкие места, о которых следует помнить.

---

## 1. Metrics Server обязателен

Во-первых, это не работает без Metrics Server. Сервер метрик должен быть запущен в Kubernetes, иметь нужные права и так далее.

Если ты собираешь кластер самостоятельно при помощи kubeadm, там его не будет.

---

## 2. Адаптеры для кастомных метрик

То же самое для пользовательских и внешних метрик - они тоже не работают без правильно настроенных адаптеров.

---

## 3. Настройка поведения масштабирования

Затем нужно будет определить поведение для масштабирования. Например, установить, на какое количество реплик мы хотим масштабироваться за один шаг, и так далее.

Тем самым мы сделаем тюнинг нашему скейлеру, то есть настроим, насколько отзывчиво он будет работать.

### Настройка поведения

Настройки задаются в поле `behavior`, и мы можем задать разные стратегии поведения для `scaleUp` и `scaleDown`.

Там можно установить вещи вроде:
- **Окна стабилизации** - сколько времени ждать перед следующим масштабированием
- **Процент изменения реплик** - на сколько процентов можно изменить реплики за раз
- **Время задержки** - через сколько времени после преодоления порога начинать действовать

### Пример настройки поведения

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 8
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 1
        periodSeconds: 60
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

---

## 4. Ресурсы должны быть настроены

Важно: без установленных настроек ресурсов (requests/limits) это не будет работать.

HPA использует ресурсные лимиты для расчета утилизации. Если они не установлены, HPA не сможет определить, нужно ли масштабирование.

---

</details>

<details>
<summary><b>🛠️Императивный способ создания HPA</b></summary>

---

## Команда kubectl autoscale

Иногда у нас нет времени и желания писать длинный манифест HPA.

`kubectl autoscale` предлагает императивный способ создания горизонтального скейлера, который может сэкономить наши трудозатраты.

Команда `autoscale` создает ресурс Horizontal Pod Autoscaler для указанного Deployment'а.

### Пример команды

```bash
kubectl autoscale deployment my-app \
  --cpu-percent=50 \
  --min=2 \
  --max=8
```

Вот здесь мы используем ее для Deployment `my-app` с настройками CPU 50 процентов и коридором граничных значений от двух до восьми реплик.

Пятидесятипроцентное условие для регулирования относится к ресурсным лимитам, которые настроены на PODе. Без установленных настроек ресурсов это не будет работать.

---

## Просмотр HPA

Получить листинг всех скейлеров можно командой:

```bash
kubectl get hpa
```

Для деталей можно использовать `describe`:

```bash
kubectl describe hpa my-app
```

Или сделать `get` в виде YAML:

```bash
kubectl get hpa my-app -o yaml
```

Как видишь, результат команды создания был идентичен разобранному ранее манифесту.

---

</details>

---

## Резюме

### Основные концепции

- **HPA** динамически регулирует количество реплик в ReplicaSet Deployment'а
- HPA патчит поле `replicas` в Deployment, а созданием/удалением PODs занимаются профильные механизмы
- HPA работает с Deployment и StatefulSet

### Компоненты HPA

1. **Определение ресурса HPA** - манифест с правилами масштабирования
2. **Metrics Server** - собирает встроенные метрики (CPU/Memory) из kubelet
3. **Адаптеры метрик** - для пользовательских и внешних метрик

### Типы метрик

1. **Resource metrics** (`metrics.k8s.io`)
   - CPU и Memory
   - Собираются через Metrics Server
   - Нативные метрики Kubernetes

2. **Custom metrics** (`custom.metrics.k8s.io`)
   - Пользовательские метрики внутри кластера
   - Требуют Custom Metrics Adapter
   - Примеры: requests_per_second, active_users

3. **External metrics** (`external.metrics.k8s.io`)
   - Внешние метрики (вне кластера)
   - Требуют External Metrics Adapter
   - Примеры: длина очереди SQS, метрики облачного провайдера

### Цикл работы HPA

1. Получение метрик (каждые 15 секунд)
2. Оценка метрик (сравнение с пороговыми значениями)
3. Расчет масштабирования
4. Обновление Deployment

### Важные моменты

- **Metrics Server обязателен** для работы HPA
- **Адаптеры обязательны** для пользовательских и внешних метрик
- **Ресурсы (requests/limits) должны быть настроены** на PODs
- Можно настроить **поведение масштабирования** (behavior) для более точного контроля
- Можно использовать **императивный способ** создания через `kubectl autoscale`

### Обработка метрик

- HPA обрабатывает метрики для каждой реплики (POD)
- В мульти-контейнерных PODs можно указать конкретный контейнер через `container`
- Для пользовательских метрик можно использовать тип `Pods` или `Object`

---

Окей, это было немного долго. Впереди тебя ждут лабораторные, а мы увидимся в следующем видео.

